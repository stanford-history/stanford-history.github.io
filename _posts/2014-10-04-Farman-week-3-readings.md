---
title: Thoughts on the OpenRefine Tutorial
date: 2014-10-04
author: Jennifer Farman
layout: post
---

The OpenRefine tutorial was an excellent introduction to the process of cleaning up messy data. However, I wish that the tutorial had gone into more detail on some steps. For instance, the tutorial had us select "blank down" in order to remove duplicate entries that had the same Record ID. However, the tutorial did not explain how this process works, leaving me wondering if this same technique would work for removing duplicates for other data sets. I also wish they had gone into more detail on the atomization section. Although the tutorial mentions that the data is atomized once we have split the multi-valued "Category" cells, they do not go into much detail on the definition or significance of "atomized" data. One frustrating challenge I ran into is that I was not able to successfully complete the "find and edit clusters" step. I made several attempts to merge similar clusters using the method they outlined but I kept getting the result "no clusters found." Despite these issues, I found the tutorial to be informative and easy to follow.

I think the tutorial did an excellent job illuminating some of the data cleanliness issues that we might run into as historians. Duplicate records, missing fields, and redundancy are all issues that often arise when using historical data sets. But there are also other issues into which this particular tutorial did not delve.  For instance, historians are likely to run into the issue of ambiguity when it comes to determining if two records are duplicates. When looking at employment records from the last 50 years, it is relatively easy to tell if two "John Smiths" are the same person. We can look at their employee ID number, or perhaps a listed address in order to confirm whether they are the same person or two distinct people with the same name. However, if we are looking at employment records from the 1600s, it may be much more difficult to tell whether two different records referencing an employee named "John Smith" are referring to the same person both times, given a lack of unique identifiers such as ID numbers.  Another issue is potentially inaccurate data. Given that many records from eras predating standardized measurements may be very roughly approximated, historians may run into trouble when attempting to use numerical measurements as part of a historical data sets. Contemporary methods of scientific / medical classifications may also cause trouble for historians. A historian researching rates of infectious disease during the 1700s would have to be mindful that many instances of disease may have been mis-classified in medical records.

It is often difficult to make messy historical data conform to the rigid requirements of digital data management systems. As was mentioned in the readings and was illuminated by the tutorial, there are a number of tools that can be used to clean up messy historical data. However, dealing with ambiguous or potentially inaccurate data is much less straight-forward, and may require the historian to make judgement call. The issue of dealing with implausible, ambiguous, or missing data is a significant issue that I think would make a great topic of discussion for this class.
